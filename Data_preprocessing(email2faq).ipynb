{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karandomguy/email-2-faq/blob/experiments/Data_preprocessing(email2faq).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k50KosVjwtOa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import jaccard_score\n",
        "cv = CountVectorizer()\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tokenize\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIVHtS4vt2lu",
        "outputId": "4b2859aa-2a89-4a2a-90f9-95d28bc9f461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting zipfile36\n",
            "  Downloading zipfile36-0.1.3-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: zipfile36\n",
            "Successfully installed zipfile36-0.1.3\n"
          ]
        }
      ],
      "source": [
        "pip install zipfile36"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j0VWUent7FK",
        "outputId": "ab2674d3-a718-44e4-8244-7488ae9b61a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id  qid1  qid2                                          question1  \\\n",
            "0   0     1     2  What is the step by step guide to invest in sh...   \n",
            "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
            "2   2     5     6  How can I increase the speed of my internet co...   \n",
            "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
            "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
            "\n",
            "                                           question2  is_duplicate  \n",
            "0  What is the step by step guide to invest in sh...             0  \n",
            "1  What would happen if the Indian government sto...             0  \n",
            "2  How can Internet speed be increased by hacking...             0  \n",
            "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
            "4            Which fish would survive in salt water?             0  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# import required modules\n",
        "import zipfile\n",
        "import pandas as pd\n",
        " \n",
        "# read the dataset using the compression zip\n",
        "df = pd.read_csv('question-pairs-dataset.zip',compression='zip')\n",
        " \n",
        "# display dataset\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Now0r1ySrMks"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0c0iwr7y10K"
      },
      "outputs": [],
      "source": [
        "df[\"question1\"]=df[\"question1\"].str.lower()\n",
        "df[\"question2\"]=df[\"question2\"].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46MNvOOGzoAv"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1qbofim1F9o"
      },
      "outputs": [],
      "source": [
        "\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6Aw_VNpVRLX"
      },
      "outputs": [],
      "source": [
        "def cleaning(text):\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\s\\W',' ',text)\n",
        "    text = re.sub('\\W,\\s',' ',text)\n",
        "    text = re.sub(r'[^\\w]', ' ', text)\n",
        "    text = re.sub(\"\\d+\", \"\", text)\n",
        "    text = re.sub('\\s+',' ',text)\n",
        "    text = re.sub('[!@#$_]', '', text)\n",
        "    text = text.replace(\"co\",\"\")\n",
        "    text = text.replace(\"https\",\"\")\n",
        "    text = text.replace(\",\",\"\")\n",
        "    text = text.replace(\"[\\w*\",\" \")\n",
        "    \n",
        "    return text\n",
        "df['question1'] = [cleaning(text) for text in df['question1']]\n",
        "df['question2'] = [cleaning(text) for text in df['question2']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1MvelNuVwfm"
      },
      "outputs": [],
      "source": [
        "newdf=df.drop(['qid1','qid2'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPdo35EyXFkj"
      },
      "outputs": [],
      "source": [
        "newdf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j80zK9v1CyRC"
      },
      "source": [
        "*Jaccard Similarity*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8GHLuZgyjb8"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(x,y):\n",
        "  \"\"\" returns the jaccard similarity between two lists \"\"\"\n",
        "  intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
        "  union_cardinality = len(set.union(*[set(x), set(y)]))\n",
        "  return intersection_cardinality/float(union_cardinality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYykKn0PyoWD"
      },
      "outputs": [],
      "source": [
        "val=0\n",
        "count=0\n",
        "for i in range(20000):\n",
        "    sentences = [newdf['question1'][i],\n",
        "    newdf['question2'][i]]\n",
        "    sentences = [sent.lower().split(\" \") for sent in sentences]\n",
        "    if jaccard_similarity(sentences[0], sentences[1])<0.5:\n",
        "      ans=True\n",
        "    else:\n",
        "      ans=False\n",
        "    if ans==newdf['is_duplicate'][i]:\n",
        "      count+=1\n",
        "print(\"accuracy=\",count/20000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "accuracy=34.6%"
      ],
      "metadata": {
        "id": "xI1IiK61Msc0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yFFEBVCC7pS"
      },
      "source": [
        "*Levenshtein Distance*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll7q-od28quN"
      },
      "outputs": [],
      "source": [
        "count=0\n",
        "def levenshtein(s1, s2):\n",
        "    if len(s1) < len(s2):\n",
        "        s1, s2 = s2, s1\n",
        "\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "\n",
        "    return previous_row[-1]/float(len(s1))\n",
        "for i in range(10000):\n",
        "  s1 = newdf['question1'][i]\n",
        "  s2 = newdf['question2'][i]\n",
        "  example_1 = (s1, s2)\n",
        "  if levenshtein(*example_1)<0.5:\n",
        "      ans=True\n",
        "  else:\n",
        "      ans=False\n",
        "  if ans==newdf['is_duplicate'][i]:\n",
        "    count+=1\n",
        "print(\"accuracy=\",count/10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "accuracy=66.5%"
      ],
      "metadata": {
        "id": "Qw9VDFLdM2az"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqhyc0a-dvPW"
      },
      "source": [
        "*Cosine Similarity*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAe3jWJdgmOy"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyI-ha09FJi3"
      },
      "outputs": [],
      "source": [
        "count=0\n",
        "# Program to measure the similarity between\n",
        "# two sentences using cosine similarity.\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# X = input(\"Enter first string: \").lower()\n",
        "# Y = input(\"Enter second string: \").lower()\n",
        "\n",
        "\n",
        "\n",
        "for i in range(50000):\n",
        "  X = newdf['question1'][i]\n",
        "  Y = newdf['question2'][i]\n",
        "\n",
        "  # tokenization\n",
        "  X_list = word_tokenize(X)\n",
        "  Y_list = word_tokenize(Y)\n",
        "\n",
        "  # sw contains the list of stopwords\n",
        "  sw = stopwords.words('english')\n",
        "  l1 =[];l2 =[]\n",
        "\n",
        "  # remove stop words from the string\n",
        "  X_set = {w for w in X_list if not w in sw}\n",
        "  Y_set = {w for w in Y_list if not w in sw}\n",
        "\n",
        "  # form a set containing keywords of both strings\n",
        "  rvector = X_set.union(Y_set)\n",
        "  for w in rvector:\n",
        "    if w in X_set: l1.append(1) # create a vector\n",
        "    else: l1.append(0)\n",
        "    if w in Y_set: l2.append(1)\n",
        "    else: l2.append(0)\n",
        "  c = 0\n",
        "\n",
        "  # cosine formula\n",
        "  for i in range(len(rvector)):\n",
        "      c+= l1[i]*l2[i]\n",
        "  if float((sum(l1)*sum(l2))**0.5)!=0:\n",
        "    cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
        "  else:\n",
        "    continue;\n",
        "  if cosine<0.5:\n",
        "      ans=True\n",
        "  else:\n",
        "      ans=False\n",
        "  if ans==newdf['is_duplicate'][i]:\n",
        "    count+=1\n",
        "print(count)\n",
        "print(\"accuracy=\",count/50000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "accuracy=58.3%"
      ],
      "metadata": {
        "id": "KsiUuzK5M6j6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM\n"
      ],
      "metadata": {
        "id": "Gx52STOGErmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "nhy9z7l_uDgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = df\n"
      ],
      "metadata": {
        "id": "pRE5hYPzxZQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df.iloc[:,:5].values\n",
        "Y_train = df.iloc[:,5:].values\n",
        "X_testq1 = test_data.iloc[:400001,1:2].values\n",
        "X_testq2 = test_data.iloc[:400001, 2:].values\n",
        "s1 = X_train[:,3]\n",
        "s2 = X_train[:,4]"
      ],
      "metadata": {
        "id": "POEITJ2FxNLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(s):\n",
        "    tokens = []\n",
        "    tokens = [word_tokenize(str(sentence)) for sentence in s]\n",
        "\n",
        "    rm1 = []\n",
        "    for w in tokens:\n",
        "        sm = re.sub('[^A-Za-z]',' ', str(w))\n",
        "        x = re.split(\"\\s\", sm)\n",
        "        rm1.append(x)\n",
        "        \n",
        "    return rm1"
      ],
      "metadata": {
        "id": "GVTesO6Qu_3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lower_case(s):\n",
        "    #Removing whitespaces    \n",
        "    for sent in s:\n",
        "        while '' in sent:\n",
        "            sent.remove('')\n",
        "\n",
        "    # Lowercasing\n",
        "    low = []\n",
        "    for i in s:\n",
        "        i = [x.lower() for x in i]\n",
        "        low.append(i)\n",
        "        \n",
        "    return low"
      ],
      "metadata": {
        "id": "IWLuSV9ovq2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(s):\n",
        "    lemma = []\n",
        "    wnl = WordNetLemmatizer()\n",
        "    for doc in s:\n",
        "        tokens = [wnl.lemmatize(w) for w in doc]\n",
        "        lemma.append(tokens)\n",
        "\n",
        "    # Removing Stopwords\n",
        "    filter_words = []\n",
        "    Stopwords = set(stopwords.words('english'))\n",
        "\n",
        "    #ab = spell('nd')\n",
        "    for sent in lemma:\n",
        "        tokens = [w for w in sent if w not in Stopwords]\n",
        "        filter_words.append(tokens)\n",
        "\n",
        "    space = ' ' \n",
        "    sentences = []\n",
        "    for sentence in filter_words:\n",
        "        sentences.append(space.join(sentence))\n",
        "        \n",
        "    return sentences"
      ],
      "metadata": {
        "id": "SsGhJkR8v0Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_NB_WORDS = 200000\n",
        "tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(list(df['question1'].values.astype(str))+list(df['question2'].values.astype(str)))"
      ],
      "metadata": {
        "id": "52COFLLKv3nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_q1 = tokenizer.texts_to_sequences(np.array(listq1))\n",
        "X_train_q1 = tokenizer.texts_to_sequences(df['question1'].values.astype(str))\n",
        "X_train_q1 = pad_sequences(X_train_q1, maxlen = 30, padding='post')\n",
        "\n",
        "# X_train_q2 = tokenizer.texts_to_sequences(np.array(listq2))\n",
        "X_train_q2 = tokenizer.texts_to_sequences(df['question2'].values.astype(str))\n",
        "X_train_q2 = pad_sequences(X_train_q2, maxlen = 30, padding='post')"
      ],
      "metadata": {
        "id": "ZPY7G2mKwDkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_q1 = tokenizer.texts_to_sequences(X_test_q1.ravel())\n",
        "X_test_q1 = pad_sequences(X_test_q1,maxlen = 30, padding='post')\n",
        "\n",
        "X_test_q2 = tokenizer.texts_to_sequences(X_test_q2.astype(str).ravel())\n",
        "X_test_q2 = pad_sequences(X_test_q2, maxlen = 30, padding='post')\n"
      ],
      "metadata": {
        "id": "us8WuG1YwG1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "-L-XH8LlwIwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_index = {}\n",
        "with open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt','r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vectors = np.asarray(values[1:], 'float32')\n",
        "        embedding_index[word] = vectors\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "SBj-T_qxwgNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.random.random((len(word_index)+1, 200))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "QjgueofIwic8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Based Semantic Similarity\n"
      ],
      "metadata": {
        "id": "urXmG29SQBt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "zCuukXYMQEdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "DNJnGXx1QFtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xN-a8xVVQLYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "MXCxLDPcQQ5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count=0\n",
        "for i in range(10000):\n",
        "  sentences = [newdf['question1'][i],\n",
        "    newdf['question2'][i]]\n",
        "  sentence1 = sentences[0]\n",
        "  sentence2 = sentences[1]\n",
        "  # encode sentences to get their embeddings\n",
        "  embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
        "  embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
        "  # compute similarity scores of two embeddings\n",
        "  cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "  if cosine_scores.item()>0.5:\n",
        "    t=1\n",
        "  else:\n",
        "    t=0\n",
        "  if t==newdf['is_duplicate'][i]:\n",
        "      count+=1\n",
        "  print(i)\n",
        "print(\"accuracy=\",count/10000)"
      ],
      "metadata": {
        "id": "8fsdGrVpQUOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "accuracy after 10000 sentences= 60.31%"
      ],
      "metadata": {
        "id": "y48EglsLYv_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WMD"
      ],
      "metadata": {
        "id": "uA7-LLnhZnLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FMrID69ZqlOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyemd"
      ],
      "metadata": {
        "id": "GZsDzqhvuMbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "    \n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('drive/My Drive/testing/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "metadata": {
        "id": "wSHh1Ts0Z9MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyemd import emd\n",
        "from gensim.similarities import WmdSimilarity\n",
        "from gensim.models.doc2vec import LabeledSentence\n",
        "from gensim.models.doc2vec import TaggedLineDocument"
      ],
      "metadata": {
        "id": "Iuu7VKI9r8hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global PYEMD_EXT\n",
        "try:\n",
        "    from pyemd import emd\n",
        "    PYEMD_EXT = True\n",
        "except ImportError:\n",
        "    PYEMD_EXT = False"
      ],
      "metadata": {
        "id": "T1j3vNyUsR6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question3 = 'Why am I mentally very lonely? How can I solve it?'\n",
        "question4 = 'Find the remainder when [math]23^{24}[/math] is divided by 24,23?'\n",
        "question3 = question3.lower().split()\n",
        "question4 = question4.lower().split()\n",
        "distance = model.WmdSimilarity(question3, question4)\n",
        "print('distance = %.4f' % distance)"
      ],
      "metadata": {
        "id": "78fjAcWqhxvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Word2Vec(sentences=common_texts)\n",
        "for i in range(10000):\n",
        "  \n",
        "  sentences = [newdf['question1'][i],\n",
        "    newdf['question2'][i]]\n",
        "  \n",
        "  sentence1 = sentences[0].lower().split()\n",
        "  sentence2 = sentences[1].lower().split()\n",
        "  \n",
        "  distance= model.wmdistance(sentence1, sentence2)"
      ],
      "metadata": {
        "id": "btNAjET-SEnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I49Gn71qZ11P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}